library(tidyverse) # for general data manipulation and graphing

setwd("~/My Files/University/Work/3rd year/BSc Research project/dissertation")
# import metadata from part 1
data <- read.table("data.csv", header = T)
# import lemmatised and tokenised texts from part 1
annotated_text_slim <- read.table("annotated_text_slim.csv", header = T)
# import document term frequency dfs from part 3
dtf_nouns <- read.table("dtf_nouns.csv", header = T)
dtf_nouns_stats <- read.table("dtf_nouns_stats.csv", header = T)

# get a list of DOIs of papers published in 2012 or before
dois_2012 <- data$DOI[data$Year < 2013]

# subset imported dfs to get just papers published in 2012 or before
annotated_text_slim_2012 <- subset(annotated_text_slim, doc_id %in% dois_2012)
dtf_nouns_2012 <- subset(dtf_nouns, doc_id %in% dois_2012)
dtf_nouns_stats_2012 <- subset(dtf_nouns_stats, doc_id %in% dois_2012)

# Build a document-term frequency matrix ----------------------------------
# load udpipe package for natural language processing tools
library(udpipe)
#method from udpipe topic modelling vignette: https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-topicmodelling.html

# build a document-term matrix from subsetted dtf
dtm_nouns_2012 <- document_term_matrix(x = dtf_nouns_2012)
# Remove words which do not occur that much
dtm_nouns_clean_2012 <- dtm_remove_lowfreq(dtm_nouns_2012, minfreq = 10)

# Topic modelling ---------------------------------------------------------
# load topicmodels package for LDA model
library(topicmodels)

model_2012 <- LDA(dtm_nouns_clean_2012, k = 12, method = "Gibbs", 
             control = list(nstart = 5, burnin = 500, best = TRUE, seed = 1:5, verbose = 20))
top10_2012 <- predict(model_2012, type = "terms", min_posterior = 0.05, min_terms = 10)
top10_2012

# Label topics ------------------------------------------------------------
# make a list of integers from 1-12
topics_2012 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
# make a list of all the topic names
labels_2012 <- c("1: PCR", "2: software", "3: cells", "4: genomes + evolution", "5: RNA", "6: statistical methods", "7: gene editing techniques", "8: results", "9: outcomes", "10: databases", "11: sequence specificity", "12: proteins")
# make a df using the integers and names
topic_labels_2012 <- data.frame(topics_2012, labels_2012) %>%
  rename(topic = topics_2012, label = labels_2012)

# load tidytext package for text mining tools, particularly tidy() function
library(tidytext)
# turn model into one-topic-per-term with probability of term being generated by topic (=beta)
model_tidy_beta_2012 <- tidy(model_2012, matrix = "beta")
# switch around topic numbers to match those in LDAvis output below
model_tidy_beta_2012$newtopic <- case_when(
                                  model_tidy_beta_2012$topic == 1 ~ 10,
                                  model_tidy_beta_2012$topic == 2 ~ 2,
                                  model_tidy_beta_2012$topic == 3 ~ 12,
                                  model_tidy_beta_2012$topic == 4 ~ 6,
                                  model_tidy_beta_2012$topic == 5 ~ 9,
                                  model_tidy_beta_2012$topic == 6 ~ 4,
                                  model_tidy_beta_2012$topic == 7 ~ 11,
                                  model_tidy_beta_2012$topic == 8 ~ 7,
                                  model_tidy_beta_2012$topic == 9 ~ 1,
                                  model_tidy_beta_2012$topic == 10 ~ 5,
                                  model_tidy_beta_2012$topic == 11 ~ 3,
                                  model_tidy_beta_2012$topic == 12 ~ 8
                                  )

# find top ten terms for each topic
top_terms_tidy_2012 <- model_tidy_beta_2012 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
# join top ten terms with the topic labels
top_terms_tidy_2012_names <- left_join(top_terms_tidy_2012, topic_labels_2012, by = c("newtopic" = "topic"))

# set up a function to name individual facets with the topic names
topic.names_2012 <- as_labeller(
  c(`1` = "1: PCR", `2` = "2: software",`3` = "3: cells", 
    `4` = "4: genomes + evolution",`5` = "5: RNA", `6` = "6: statistical methods",
    `7` = "7: gene editing techniques", `8` = "8: results", `9` = "9: outcomes",
    `10` = "10: databases", `11` = "11: sequence specificity", `12` = "12: proteins"))

# plot the top ten terms per topic using beta values and facet by newtopic to give 12 individual charts arranged as one
ggplot(top_terms_tidy_2012_names, aes(beta, reorder_within(term, beta, newtopic))) +
  geom_col(show.legend = FALSE, fill = "#28527f") +
  facet_wrap(~ newtopic, scales = "free", labeller = topic.names_2012) +
  scale_y_reordered() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), strip.text.x = element_text(size = 15, face = "bold")) +
  xlab("Beta value") +
  ylab("Term") +
  theme(text = element_text(size = 18))

# write data to file  
write.table(top_terms_tidy_2012_names, "top_terms_tidy_2012_names.csv")

# examining the per-document-per-topic probabilities, gamma
model_tidy_gamma_2012 <- tidy(model_2012, matrix = "gamma")
# reshuffle topic numbers to match those in LDAvis, below
model_tidy_gamma_2012$newtopic <- case_when(
  model_tidy_gamma_2012$topic == 1 ~ 10,
  model_tidy_gamma_2012$topic == 2 ~ 2,
  model_tidy_gamma_2012$topic == 3 ~ 12,
  model_tidy_gamma_2012$topic == 4 ~ 6,
  model_tidy_gamma_2012$topic == 5 ~ 9,
  model_tidy_gamma_2012$topic == 6 ~ 4,
  model_tidy_gamma_2012$topic == 7 ~ 11,
  model_tidy_gamma_2012$topic == 8 ~ 7,
  model_tidy_gamma_2012$topic == 9 ~ 1,
  model_tidy_gamma_2012$topic == 10 ~ 5,
  model_tidy_gamma_2012$topic == 11 ~ 3,
  model_tidy_gamma_2012$topic == 12 ~ 8
)
write.table(model_tidy_gamma_2012, "model_tidy_gamma_2012.csv")

# Word clouds for each topic ----------------------------------------------
model_posterior_2012 <- posterior(model_2012)
# visualize topics as word cloud
topicToViz_2012 <- 5 # change for your own topic of interest
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms_2012 <- sort(model_posterior_2012$terms[topicToViz_2012,], decreasing=TRUE)[1:40]
words_2012 <- names(top40terms_2012)
# extract the probabilites of each of the 40 terms
probabilities_2012 <- sort(model_posterior_2012$terms[topicToViz_2012,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
wordcloud2(data.frame(words_2012, probabilities_2012), shuffle = FALSE, size = 0.8)


# Using LDAvis package -----------------------------------------------------
# load LDAvis package for interactive visualisation of LDA topic model
library(LDAvis)

# matrix of phi values (probability of a specified word occurring in a specified topic)
phi_2012 <- posterior(model_2012)$terms %>% 
  as.matrix
# matrix of theta values (probability of a specified topic occurring in a specified document)
theta_2012 <- posterior(model_2012)$topics %>% 
  as.matrix
#list of all the tokens (nouns) used
vocab_2012 <- colnames(phi_2012) 
#number of tokens (nouns) per document
doc.length_2012 <- dtf_nouns_2012 %>% 
  group_by(doc_id) %>%
  count()
#number of times each term is used overall
term.frequency_2012 <- dtf_nouns_2012 %>% 
  ungroup() %>%
  filter(term %in% dtm_nouns_clean_2012@Dimnames[[2]]) %>%
  group_by(term) %>%
  count()

# generate interactive viz using objects above
json_2012 <- createJSON(phi = phi_2012, 
                   theta = theta_2012, 
                   doc.length = doc.length_2012$n, 
                   vocab = vocab_2012, 
                   term.frequency = term.frequency_2012$n)
json_list_2012 <- list(phi_2012, theta_2012, doc.length_2012$n, vocab_2012, term.frequency_2012$n)
serVis(json_2012, out.dir = 'vis', open.browser = T)
# use this in Shiny: https://raw.githubusercontent.com/cpsievert/LDAvis/master/inst/examples/rmarkdown.Rmd
# https://github.com/cpsievert/LDAvis/issues/27
save(json_list_2012, file = "json_list_2012.RData")

# Looking at random paper and its topics -------------------------------------
# randomly select one of the paper DOIs
paper_2012 <- model_tidy_gamma_2012[sample(nrow(model_tidy_gamma_2012), 1), ]$document
# extract the topics and gamma values associated with that paper
paper_2012_topics <- filter(model_tidy_gamma_2012, document == paper_2012) %>%
  arrange(desc(gamma)) %>%
  left_join(topic_labels_2012)
# extract the title associated with that paper
paper_2012_title <- filter(metadata, DI == paper_2012)$TI
# extract the abstract associated with that paper
paper_2012_abstract <- filter(metadata, DI == paper_2012)$AB
# make a df using the topics and gamma values previously extracted
paper_2012_topics_gamma <- data.frame(paper_2012_topics$label, paper_2012_topics$gamma) %>%
  rename(topic = paper_2012_topics.label, gamma = paper_2012_topics.gamma)

# Create table for supp figures -------------------------------------------
# as above but paper DOI is specified rather than chosen randomly
paper_example_2012 <- "10.1186/1471-2148-6-55"
paper_example_2012_topics <- filter(model_tidy_gamma_2012, document == paper_example_2012) %>%
  arrange(desc(gamma)) %>%
  left_join(topic_labels_2012, by = c("newtopic" = "topic"))
paper_example_2012_title <- filter(metadata, DI == paper_example_2012)$TI
paper_example_2012_topics_gamma <- data.frame(paper_example_2012_topics$label, paper_example_2012_topics$gamma) %>%
  rename(Topic = paper_example_2012_topics.label, Gamma = paper_example_2012_topics.gamma)

# load gt package for table creation
library(gt)
paper_example_2012_topics_gamma <- paper_example_2012_topics_gamma %>%
  gt()
# load webshot package to save image of gt table
library(webshot)
gtsave(paper_example_2012_topics_gamma, "paper_example_2012_topics_gamma.png")