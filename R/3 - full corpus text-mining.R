library(tidyverse) # for general data manipulation and graphing
library(wordcloud2) # for word cloud generation

setwd("~/My Files/University/Work/3rd year/BSc Research project/dissertation")
# import metadata from part 1
data <- read.table("data.csv", header = T)
# import lemmatised and tokenised texts from part 1
annotated_text_slim <- read.table("annotated_text_slim.csv", header = T)

# count how many of each word
full_word_count <- count(annotated_text_slim, word, sort = TRUE) 
# name the column of counts 'freq'
names(full_word_count)[2] <- "freq" 
# generate a word cloud
wordcloud2(full_word_count, color = "random-light", backgroundColor = "white") 

# Build a document-term frequency matrix ----------------------------------
# load udpipe package for natural language processing tools
library(udpipe)
#method from udpipe topic modelling vignette: https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-topicmodelling.html

# build document-term frequency df using nouns only
dtf_nouns <- subset(annotated_text_slim, upos %in% c("NOUN")) %>%
  document_term_frequencies(dtf_nouns, document = "doc_id", term = "word")
# build a df with further statistics about term frequencies
dtf_nouns_stats <- document_term_frequencies_statistics(dtf_nouns) %>%
  arrange(desc(tf_idf))

# build a document-term matrix
dtm_nouns <- document_term_matrix(x = dtf_nouns)
# Remove words which do not occur that much
dtm_nouns_clean <- dtm_remove_lowfreq(dtm_nouns, minfreq = 10)

write.table(dtf_nouns, "dtf_nouns.csv")
write.table(dtf_nouns_stats, "dtf_nouns_stats.csv")

# Topic modelling ---------------------------------------------------------
# load topicmodels package for LDA model
library(topicmodels)

model <- LDA(dtm_nouns_clean, k = 12, method = "Gibbs", 
         control = list(nstart = 5, burnin = 500, best = TRUE, seed = 1:5, verbose = 20))
top10 <- predict(model, type = "terms", min_posterior = 0.05, min_terms = 10)

# load tidytext package for text mining tools, particularly tidy() function
library(tidytext)
#turn model into one-topic-per-term with probability of term being generated by topic (=beta)
model_tidy_beta <- tidy(model, matrix = "beta")
#find top ten terms for each topic
top_terms_tidy <- model_tidy_beta %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
top_terms_tidy %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# load tidyr package for tidying/rearranging data with spread()
library(tidyr)

beta_spread <- model_tidy_beta %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

# examining the per-document-per-topic probabilities, gamma
model_tidy_gamma <- tidy(model, matrix = "gamma")
model_tidy_gamma

# Word clouds for each topic ----------------------------------------------
model_posterior <- posterior(model)
# visualize topics as word cloud
topicToViz <- 5 # change for your own topic of interest
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms <- sort(model_posterior$terms[topicToViz,], decreasing=TRUE)[1:40]
words <- names(top40terms)
# extract the probabilites of each of the 40 terms
probabilities <- sort(model_posterior$terms[topicToViz,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
wordcloud2(data.frame(words, probabilities), shuffle = FALSE, size = 0.8)

# Using LDAvis package -----------------------------------------------------
# load LDAvis package for interactive visualisation of LDA topic model
library(LDAvis)

# matrix of phi values (probability of a specified word occurring in a specified topic)
phi <- posterior(model)$terms %>% 
  as.matrix
# matrix of theta values (probability of a specified topic occurring in a specified document)
theta <- posterior(model)$topics %>% 
  as.matrix
# list of all the tokens (nouns) used
vocab <- colnames(phi) 
# number of tokens (nouns) per document
doc.length <- dtf_nouns %>% 
  group_by(doc_id) %>%
  count()
# number of times each term is used overall
term.frequency <- dtf_nouns %>% 
  ungroup() %>%
  filter(term %in% dtm_nouns_clean@Dimnames[[2]]) %>%
  group_by(term) %>%
  count()

# generate interactive viz using objects above
json <- createJSON(phi = phi, 
                   theta = theta, 
                   doc.length = doc.length$n, 
                   vocab = vocab, 
                   term.frequency = term.frequency$n)
json_list <- list(phi, theta, doc.length$n, vocab, term.frequency$n)
serVis(json, out.dir = 'vis', open.browser = T)
# use this in Shiny: https://raw.githubusercontent.com/cpsievert/LDAvis/master/inst/examples/rmarkdown.Rmd
# https://github.com/cpsievert/LDAvis/issues/27
