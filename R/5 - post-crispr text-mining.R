library(tidyverse) # for general data manipulation and graphing

setwd("~/My Files/University/Work/3rd year/BSc Research project/dissertation")
# import metadata from part 1
data <- read.table("data.csv", header = T)
# import lemmatised and tokenised texts from part 1
annotated_text_slim <- read.table("annotated_text_slim.csv", header = T)
# import document term frequency dfs from part 3
dtf_nouns <- read.table("dtf_nouns.csv", header = T)
dtf_nouns_stats <- read.table("dtf_nouns_stats.csv", header = T)

# get a list of DOIs of papers published in 2020
dois_2020 <- data$DOI[data$Year == 2020]

# subset imported dfs to get just papers published in 2012 or before
annotated_text_slim_2020 <- subset(annotated_text_slim, doc_id %in% dois_2020) #hasnt worked
dtf_nouns_2020 <- subset(dtf_nouns, doc_id %in% dois_2020)
dtf_nouns_stats_2020 <- subset(dtf_nouns_stats, doc_id %in% dois_2020)

# Build a document-term frequency matrix ----------------------------------
# load udpipe package for natural language processing tools
library(udpipe)
#method from udpipe topic modelling vignette: https://cran.r-project.org/web/packages/udpipe/vignettes/udpipe-usecase-topicmodelling.html

# build a document-term matrix
dtm_nouns_2020 <- document_term_matrix(x = dtf_nouns_2020)
# Remove words which do not occur that much
dtm_nouns_clean_2020 <- dtm_remove_lowfreq(dtm_nouns_2020, minfreq = 10)

# Topic modelling ---------------------------------------------------------
# load topicmodels package for LDA model
library(topicmodels)

model_2020 <- LDA(dtm_nouns_clean_2020, k = 12, method = "Gibbs", 
                  control = list(nstart = 5, burnin = 500, best = TRUE, seed = 1:5, verbose = 20))
top10_2020 <- predict(model_2020, type = "terms", min_posterior = 0.05, min_terms = 10)

# Label topics ------------------------------------------------------------
# make a list of integers from 1-12
topics_2020 <- c(1,2,3,4,5,6,7,8,9,10,11,12)
# make a list of all the topic names
labels_2020 <- c("1: CRISPR-Cas9", "2: sequencing", "3: signal transduction pathways", "4: antibodies", "5: gene editing", "6: therapies", "7: discussion", "8: sequences", "9: protein assays", "10: animal models", "11: cell culture", "12: plants")
# make a df using the integers and names
topic_labels_2020 <- data.frame(topics_2020, labels_2020) %>%
  rename(topic = topics_2020, label = labels_2020)

# load tidytext package for text mining tools, particularly tidy() function
library(tidytext)
# turn model into one-topic-per-term with probability of term being generated by topic (=beta)
model_tidy_beta_2020 <- tidy(model_2020, matrix = "beta")
# switch around topic numbers to match those in LDAvis output below
model_tidy_beta_2020$newtopic <- case_when(
                                  model_tidy_beta_2020$topic == 1 ~ 3,
                                  model_tidy_beta_2020$topic == 2 ~ 5,
                                  model_tidy_beta_2020$topic == 3 ~ 11,
                                  model_tidy_beta_2020$topic == 4 ~ 12,
                                  model_tidy_beta_2020$topic == 5 ~ 8,
                                  model_tidy_beta_2020$topic == 6 ~ 10,
                                  model_tidy_beta_2020$topic == 7 ~ 1,
                                  model_tidy_beta_2020$topic == 8 ~ 9,
                                  model_tidy_beta_2020$topic == 9 ~ 6,
                                  model_tidy_beta_2020$topic == 10 ~ 7,
                                  model_tidy_beta_2020$topic == 11 ~ 4,
                                  model_tidy_beta_2020$topic == 12 ~ 2)
#find top ten terms for each topic
top_terms_tidy_2020 <- model_tidy_beta_2020 %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
# join top ten terms with the topic labels
top_terms_tidy_2020_names <- left_join(top_terms_tidy_2020, topic_labels_2020, by = c("newtopic" = "topic"))

# set up a function to name individual facets with the topic names
topic.names_2020 <- as_labeller(
  c(`1` = "1: CRISPR-Cas9", `2` = "2: sequencing",`3` = "3: signal transduction \npathways", 
    `4` = "4: antibodies",`5` = "5: gene editing", `6` = "6: therapies",
    `7` = "7: discussion", `8` = "8: sequences", `9` = "9: protein assays",
    `10` = "10: animal models", `11` = "11: cell culture", `12` = "12: plants"))

# plot the top ten terms per topic using beta values and facet by newtopic to give 12 individual charts arranged as one
ggplot(top_terms_tidy_2020_names, aes(beta, reorder_within(term, beta, newtopic))) +
  geom_col(show.legend = FALSE, fill = "#28527f") +
  facet_wrap(~ newtopic, scales = "free", labeller = topic.names_2020) +
  scale_y_reordered() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1), strip.text.x = element_text(size = 15, face = "bold")) +
  xlab("Beta value") +
  ylab("Term") +
  theme(text = element_text(size = 18))

# write data to file
write.table(top_terms_tidy_2020_names, "top_terms_tidy_2020_names.csv")

# examining the per-document-per-topic probabilities, gamma
model_tidy_gamma_2020 <- tidy(model_2020, matrix = "gamma")
# reshuffle topic numbers to match those in LDAvis, below
model_tidy_gamma_2020$newtopic <- case_when(
  model_tidy_gamma_2020$topic == 1 ~ 3,
  model_tidy_gamma_2020$topic == 2 ~ 5,
  model_tidy_gamma_2020$topic == 3 ~ 11,
  model_tidy_gamma_2020$topic == 4 ~ 12,
  model_tidy_gamma_2020$topic == 5 ~ 8,
  model_tidy_gamma_2020$topic == 6 ~ 10,
  model_tidy_gamma_2020$topic == 7 ~ 1,
  model_tidy_gamma_2020$topic == 8 ~ 9,
  model_tidy_gamma_2020$topic == 9 ~ 6,
  model_tidy_gamma_2020$topic == 10 ~ 7,
  model_tidy_gamma_2020$topic == 11 ~ 4,
  model_tidy_gamma_2020$topic == 12 ~ 2)
write.table(model_tidy_gamma_2020, "model_tidy_gamma_2020.csv")

# Word clouds for each topic ----------------------------------------------
model_posterior_2020 <- posterior(model_2020)
# visualize topics as word cloud
topicToViz_2020 <- 5 # change for your own topic of interest
# select to 40 most probable terms from the topic by sorting the term-topic-probability vector in decreasing order
top40terms_2020 <- sort(model_posterior_2020$terms[topicToViz_2020,], decreasing=TRUE)[1:40]
words_2020 <- names(top40terms_2020)
# extract the probabilites of each of the 40 terms
probabilities_2020 <- sort(model_posterior_2020$terms[topicToViz_2020,], decreasing=TRUE)[1:40]
# visualize the terms as wordcloud
wordcloud2(data.frame(words_2020, probabilities_2020), shuffle = FALSE, size = 0.8)

# Using LDAvis package -----------------------------------------------------
# load LDAvis package for interactive visualisation of LDA topic model
library(LDAvis)

# matrix of phi values (probability of a specified word occurring in a specified topic)
phi_2020 <- posterior(model_2020)$terms %>% 
  as.matrix
# matrix of theta values (probability of a specified topic occurring in a specified document)
theta_2020 <- posterior(model_2020)$topics %>% 
  as.matrix
#list of all the tokens (nouns) used
vocab_2020 <- colnames(phi_2020) 
#number of tokens (nouns) per document
doc.length_2020 <- dtf_nouns_2020 %>% 
  group_by(doc_id) %>%
  count()
#number of times each term is used overall
term.frequency_2020 <- dtf_nouns_2020 %>% 
  ungroup() %>%
  filter(term %in% dtm_nouns_clean_2020@Dimnames[[2]]) %>%
  group_by(term) %>%
  count()

# generate interactive viz using objects above
json_2020 <- createJSON(phi = phi_2020, 
                        theta = theta_2020, 
                        doc.length = doc.length_2020$n, 
                        vocab = vocab_2020, 
                        term.frequency = term.frequency_2020$n)
json_list_2020 <- list(phi_2020, theta_2020, doc.length_2020$n, vocab_2020, term.frequency_2020$n)
serVis(json_2020, out.dir = 'vis', open.browser = T)
# use this in Shiny: https://raw.githubusercontent.com/cpsievert/LDAvis/master/inst/examples/rmarkdown.Rmd
# https://github.com/cpsievert/LDAvis/issues/27

save(json_list_2020, file = "json_list_2020.RData")

# Looking at one paper and its topics -------------------------------------
# randomly select one of the paper DOIs
paper_2020 <- model_tidy_gamma_2020[sample(nrow(model_tidy_gamma_2020), 1), ]$document
# extract the topics and gamma values associated with that paper
paper_2020_topics <- filter(model_tidy_gamma_2020, document == paper_2020) %>%
  arrange(desc(gamma)) %>%
  left_join(topic_labels_2020)
# extract the title associated with that paper
paper_2020_title <- filter(metadata, DI == paper_2020)$TI
# extract the abstract associated with that paper
paper_2020_abstract <- filter(metadata, DI == paper_2020)$AB
# make a df using the topics and gamma values previously extracted
paper_2020_topics_gamma <- data.frame(paper_2020_topics$label, paper_2020_topics$gamma) %>%
  rename(topic = paper_2020_topics.label, gamma = paper_2020_topics.gamma)

# Create table for supp figures -------------------------------------------
# as above but paper DOI is specified rather than chosen randomly
paper_example_2020 <- "10.1016/j.omtm.2020.04.018"
paper_example_2020_topics <- filter(model_tidy_gamma_2020, document == paper_example_2020) %>%
  arrange(desc(gamma)) %>%
  left_join(topic_labels_2020, by = c("newtopic"="topic"))
paper_example_2020_title <- filter(metadata, DI == paper_example_2020)$TI
paper_example_2020_topics_gamma <- data.frame(paper_example_2020_topics$label, paper_example_2020_topics$gamma) %>%
  rename(Topic = paper_example_2020_topics.label, Gamma = paper_example_2020_topics.gamma)

# load gt package for table creation
library(gt)
paper_example_2020_topics_gamma <- paper_example_2020_topics_gamma %>%
  gt()
# load webshot package to save image of gt table
library(webshot)
gtsave(paper_example_2020_topics_gamma, "paper_example_2020_topics_gamma.png")